## Techniques and challenges for benchmarking methods

Visualizations and biological assessment of marker gene lists
resulting from multi-omics analyses provide critical interpretation of
integrative analysis of high-throughput data, but additional
quantitative metrics are necessary to delineate biologically-relevant
features from features arising from either computational or
technical artifacts. Beyond interpretation, quantitative benchmarks
are also essential to enable unbiased comparisons between analytical
methods. For example, the goal of multi-platform single cell data
analysis is often the recovery of known cell types through
computational methods, where metrics such as the adjusted Rand Index
(ARI) enable direct assessment of the clustering results with respect
to known cell types.  When cell types or biological features are not
known *a priori*, benchmark methods can also be used to discover known
relationships between data modalities, for example *cis* gene
regulatory mechanisms observed between chromatin accessibility and
gene expression. However, as many of these relationships are not fully understood
at the single cell level, it is essential that benchmark standards are
set in place for validation (Figure {@fig:benchmark}A).
  
![](images/Benchmark2.png){#fig:benchmark}
Caption figure: **A** Systematic benchmarking of single cell multi-omic analysis methods can involve experimental data (as per the hackathon challenges in this workshop), custom control datasets, where known structure is imposed through the experimental design, or by simulating data. The amount of biological signal and ground truth available varies considerably between these types of data. The resulting multi-omics datasets are analysed by competing methods and compared using metrics that are general purpose or take account of the ground truth (e.g. cell type labels or number of cell types simulated).
**B** scNMT-seq study: correlations with linear projections (MOFA+) evaluated with cross-validation.


### Strategies for benchmarking

Benchmarking multi-modal methods is inherently difficult, as ground
truth is rarely known. Ground truth can be introduced through simulating
high-throughput data *in silico*, but in the context of data integration,
the simulation of a realistic covariance structure across features and
across data modalities is challenging
[@doi:10.1093/bioinformatics/bty1054] and must rely on an underlying
generative model that may introduce further biases into the benchmarking
analysis. Another strategy is to use cross-validation within a study,
or conduct cross-study validation to assess whether solutions found by
multi-modal methods generalize to held-out observations or held-out
studies. The latter was somewhat attempted in the spatial proteomics
cross-study hackathon, but where ground truth was unknown
(@ref{sec:proteomics}).

<!--. Moreover, these simulations can inadvertently embed the same underlying assumptions as the computational methods employed for analysis, introducing further biases into benchmark studies. Therefore, high-throughput datasets with a known ground truth are also critical for multi-omics studies and robust testgrounds for future hackathon studies that were widely discussed throughout the workshop.-->

#### Creating benchmarking datasets

Benchmark datasets for single cell studies have largely focused on  measuring sequencing depth and diversity of cell types derived from a single assay of interest (e.g. scRNAseq). Common experimental designs involve creating artificial samples through the mixing of cells in known proportions [@doi:10.1038/s41592-019-0425-8; @doi:10.1038/s41587-020-0469-4; @doi:10.1038/s41587-020-0465-8] or creating dilution series to simulate variation in cell size [@doi:10.1038/nmeth.2645; @doi:10.1038/s41592-019-0425-8]. Simulating data is also popular and made more convenient through software such as the splatter R package [@doi:10.1186/s13059-017-1305-0]. A benchmark dataset serves two main purposes: to provide ground truth for the intended effect of exposure in a
proposed study design, and to provide validation for a data integration task for which a new computational method may be proposed.

For multi-modal assays, while the intended effects can vary based on the leading biological questions, one may abstract out common data integration tasks such as co-embedding, mapping or correlation, and inferring causal relationships. We distinguish data integration from further downstream analyses that may occur on integrated samples such as differential analysis of both assays with regard to a certain exposure.

Both the intended effects and data integration task rely on study design that takes into account the biological and technical variability via replicates, block
design, and randomization, the power analysis for the intended effect or data integration task, and the dependencies between modalities, for e.g. gene expression
depending on gene regulatory element activity, requires that experiment design must also account for spatial and temporal elements in sampling for a given observation.

As such, no universal benchmark data scheme may suit every combination of modality (e.g. the mixture design does not generalise to the spatial context), and benchmark datasets may be established for commonly used combinations of modalities or technologies, towards specific data integration tasks.
  
  
#### Cross-validation within study

As mentioned above, cross-validation within a representative
multi-modal study is one possible approach for quantitative assessment
for unbiased comparison of methods. We note that the approach of
cross-validation -- in which observations are split into folds or left
out individually for assessing model fit -- has been used often for
parameter tuning within methods, or for other aspects of model selection
[@doi:10.2202/1544-6115.1390; @doi:10.1016/j.jmva.2007.06.007;
@doi:10.2202/1544-6115.1329; @doi:10.18637/jss.v023.i12;
@doi:10.1142/S0218339009002831; @doi:10.1093/biostatistics/kxp008;
@doi:10.2202/1544-6115.1406; @doi:10.1186/1471-2105-11-191;
@doi:10.1186/1471-2164-13-160; @doi:10.1093/bioinformatics/bts476;
@doi:10.1371/journal.pcbi.1005752;
@doi:10.1093/bioinformatics/bty1054].  
Similarly, permutation has been used to create null datasets, either
as demonstration that a particular method is not overfitting, or for
parameter tuning, where the optimal parameter setting should result in
a model score that is far from the null distribution of model scores
[@doi:10.1186/1471-2105-4-59;
@doi:10.2202/1544-6115.1470; @doi:10.1074/mcp.TIR118.001251].
Cross-validation is particularly useful as a quantitative assessment
of a method's self-consistency, even though it cannot determine the
*accuracy* of a method in a completely unbiased way, if we do not have
access to an external test data set for further confirmation. 

A cross-validation analysis of the scNMT-seq dataset using MOFA+ was
performed as part of the third hackathon, and demonstrated that strong
relationships found among pairs of modalities in single cells used
for training the model were likewise often found equally strong
in held out cells (Figure {@fig:benchmark}B).  One challenge that this
hackathon attempt revealed was how to reliably match dimensions of
latent space across cross-validation folds. Previous evaluations of
multi-modal methods have focused only on the top ``latent factor''
[@doi:10.1093/bib/bbz070], however, as was seen in the evaluation of
MOFA+ on the scNMT-seq dataset, many latent factors can be reliably
discovered in held out cells in studies of complex biological
processes such as differentiation of embryonic cells.

For clustering assessment, several studies have used resampling or
data-splitting strategies to determine prediction strength
[@doi:10.1073/pnas.161273698;
@doi:10.1186/gb-2002-3-7-research0036; @doi:10.1198/106186005X59243;
@doi:10.1093/jnci/djr545]. These techniques could be further extended
in a multi-modal setting for clustering of cells into putative cell
types or cell states. Community-based benchmarking efforts in the
area of multi-modal data analysis could follow the paradigm of
the [DREAM Challenges](http://dreamchallenges.org/), with multi-modal
training data provided and test samples held out, in order to evaluate
the method submissions from participating groups.

<!--
Mike: I will add the vignettes refs in a supp table and cross refer appropriately
[cross-validation analysis of the scNMT-seq dataset](https://mikelove.github.io/BIRSBIO2020.Benchmarking.CVmofa/)
was performed as part of the hackathon for this meeting using MOFA+ (Figure {@fig:benchmark}B).
-->

#### Cross-validation between studies

Our benchmarking hackathons have emphasized the need to access
external studies for methods assessment and validation, where either
the ground truth is based on biological knowledge of the system being 
studied, or via high-quality control experiments where the ground truth 
(e.g. cell type labels) are known (Figure {@fig:benchmark}A). 
To take advantage of all data and technologies available, cross-study 
validation could also extend to cross-platform, to assess whether 
relationships discovered in one dataset are present in other datasets, 
looking across single cell and bulk as was recently proposed in [@doi:10.1101/2020.03.09.984468].

